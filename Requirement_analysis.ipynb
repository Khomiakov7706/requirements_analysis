{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khomiakov7706/requirements_analysis/blob/develop/Requirement_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "0da7ec38",
      "metadata": {
        "id": "0da7ec38"
      },
      "outputs": [],
      "source": [
        "#@title libraries import\n",
        "\n",
        "#import os\n",
        "#import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "#ML libraries\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn import metrics\n",
        "\n",
        "import pickle\n",
        "\n",
        "plt.rcParams.update({'figure.max_open_warning': 0})\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.set_option('display.max_columns', 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Чтение и обработка начального текста"
      ],
      "metadata": {
        "id": "bSXplQqbTEjr"
      },
      "id": "bSXplQqbTEjr"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data URL. { display-mode: \"form\" }\n",
        "data_url = 'https://raw.githubusercontent.com/Khomiakov7706/requirements_analysis/main/train_test_withcols1.csv' #@param {type: 'string'}"
      ],
      "metadata": {
        "id": "Pcbg_k65Z4qz"
      },
      "id": "Pcbg_k65Z4qz",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3af6906a",
      "metadata": {
        "id": "3af6906a"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2552afa7",
      "metadata": {
        "id": "2552afa7"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['req_type', 'test_scenario'], axis=1)\n",
        "\n",
        "#@title remove frase 'Verification of ' from requirements classes titles\n",
        "# Iterate over the columns in the DataFrame\n",
        "for column in data.columns:\n",
        "    # Replace the phrase \"Verification of \" with an empty string\n",
        "    new_column_name = column.replace(\"Verification of \", \"\")\n",
        "    \n",
        "    # Rename the column in the DataFrame\n",
        "    data.rename(columns={column: new_column_name}, inplace=True)\n",
        "\n",
        "data.rename(columns={'requirement': 'requirement_text',\n",
        "                     'the Authentication and Authorization process': 'Authentication and Authorization',\n",
        "                     'the Payment flow and details': 'Payment flow and details',\n",
        "                     'the Product Page and Products': 'Product Page and Products',\n",
        "                     'Security and Privacy Policy Set Up': 'Security and Privacy Policy',\n",
        "                     'Integration, Maintenance of the System and Web Standards': 'Integration'\n",
        "                     }, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cc80dd",
      "metadata": {
        "id": "10cc80dd"
      },
      "outputs": [],
      "source": [
        "requirement_classes = list(data.columns[2:].values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['classes_count'] = data.iloc[:, 2:].astype(bool).sum(axis=1)"
      ],
      "metadata": {
        "id": "bjjevS_EMhsU"
      },
      "id": "bjjevS_EMhsU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ff0abf6",
      "metadata": {
        "id": "0ff0abf6"
      },
      "source": [
        "# Лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e669277",
      "metadata": {
        "id": "2e669277"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcde1a6b",
      "metadata": {
        "id": "bcde1a6b"
      },
      "outputs": [],
      "source": [
        "def lemmatization(text):\n",
        "    '''a function for lemmatization'''\n",
        "    text = [lemmatizer.lemmatize(word.lower()) for word in text.split()]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b2a0b2",
      "metadata": {
        "id": "22b2a0b2"
      },
      "outputs": [],
      "source": [
        "data['lemmatized_requirement'] = data['requirement_text'].apply(lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a7bd3c",
      "metadata": {
        "id": "a1a7bd3c"
      },
      "source": [
        "Убираем стоп-слова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63715691",
      "metadata": {
        "id": "63715691"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4303ceb0",
      "metadata": {
        "id": "4303ceb0"
      },
      "outputs": [],
      "source": [
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words \n",
        "    text = [word for word in text.split() if word not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38ba917",
      "metadata": {
        "id": "d38ba917"
      },
      "outputs": [],
      "source": [
        "data['lemmatized_requirement'] = data['lemmatized_requirement'].apply(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b9cb31",
      "metadata": {
        "id": "58b9cb31"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = re.compile('[^a-zA-Z а-яА-Я]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ddfef32",
      "metadata": {
        "id": "7ddfef32"
      },
      "outputs": [],
      "source": [
        "data.lemmatized_requirement = data.lemmatized_requirement.apply(lambda x: regex.sub('',x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6176b0",
      "metadata": {
        "id": "ab6176b0"
      },
      "source": [
        "# Выявление признаков из датасета"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0bf1e5",
      "metadata": {
        "id": "4d0bf1e5"
      },
      "source": [
        "## Синтетические признаки"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34944766",
      "metadata": {
        "id": "34944766"
      },
      "source": [
        "составляем набор признаков на основе текста Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23cabd01",
      "metadata": {
        "id": "23cabd01"
      },
      "source": [
        "Количество слов в тексте требования "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cbae11b",
      "metadata": {
        "id": "0cbae11b"
      },
      "outputs": [],
      "source": [
        "data['words_in_requirement'] = (\n",
        "    data['requirement_text'].str.split().apply(len))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5e17ad",
      "metadata": {
        "id": "2b5e17ad"
      },
      "source": [
        "Кол-во слов в тексте лемматизированного требования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41a579c",
      "metadata": {
        "id": "b41a579c"
      },
      "outputs": [],
      "source": [
        "data['words_in_lemmatized_req'] = (\n",
        "    data['lemmatized_requirement'].str.split().apply(len))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccdd3d2",
      "metadata": {
        "id": "6ccdd3d2"
      },
      "source": [
        "Кол-во исключенных слов при лемматизации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf692c3",
      "metadata": {
        "id": "ccf692c3"
      },
      "outputs": [],
      "source": [
        "data['words_diffence'] = (\n",
        "    data['words_in_requirement']-data['words_in_lemmatized_req'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58b11bcb",
      "metadata": {
        "id": "58b11bcb"
      },
      "outputs": [],
      "source": [
        "additional_parameters = data.columns[-3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb8d787",
      "metadata": {
        "id": "ceb8d787"
      },
      "source": [
        "## Формирование Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671a88bb",
      "metadata": {
        "id": "671a88bb"
      },
      "source": [
        "Составляем словарь слов для разных классов с указанием количества используемых слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5d4e63",
      "metadata": {
        "id": "fc5d4e63"
      },
      "outputs": [],
      "source": [
        "class_dict = dict()\n",
        "for c in range(len(requirement_classes)):\n",
        "    class_dict[requirement_classes[c]] = data[data[requirement_classes[c]]==1]['lemmatized_requirement'].str.split(expand=True).unstack().value_counts().head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7440662a",
      "metadata": {
        "id": "7440662a"
      },
      "source": [
        "Составляем мешок уникальных слов для каждого класса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecee6530",
      "metadata": {
        "id": "ecee6530"
      },
      "outputs": [],
      "source": [
        "bag_of_word = []\n",
        "for i in requirement_classes:\n",
        "    for j in class_dict[i].index:\n",
        "        if not (j in bag_of_word):\n",
        "            bag_of_word.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "id": "kFH_xmGoNuOp"
      },
      "id": "kFH_xmGoNuOp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "351bab4f",
      "metadata": {
        "id": "351bab4f"
      },
      "source": [
        "Выписываем наиболее используемые слова как признаки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c07c54a",
      "metadata": {
        "id": "0c07c54a"
      },
      "outputs": [],
      "source": [
        "bow_data = data.copy()\n",
        "for word in bag_of_word:\n",
        "    bow_data[word]=bow_data['lemmatized_requirement'].apply(lambda x: x.count(str (' ' + word + ' ')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow_data.head(1)"
      ],
      "metadata": {
        "id": "yM_V4w19OCMu"
      },
      "id": "yM_V4w19OCMu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e98fa432",
      "metadata": {
        "id": "e98fa432"
      },
      "source": [
        "Создаем разметку данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec22795",
      "metadata": {
        "id": "0ec22795"
      },
      "outputs": [],
      "source": [
        "target = requirement_classes\n",
        "features = bag_of_word\n",
        "for i in additional_parameters:\n",
        "    features.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48136d97",
      "metadata": {
        "id": "48136d97"
      },
      "source": [
        "# Разбиение на тестовую и валидационную выборки для каждого класса требований для Bag of words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ebf67f2",
      "metadata": {
        "id": "2ebf67f2"
      },
      "source": [
        "Всего 26 классов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b15bada",
      "metadata": {
        "id": "7b15bada"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb938106",
      "metadata": {
        "id": "bb938106"
      },
      "outputs": [],
      "source": [
        "def split_train_test (dataframe_, target_, features_):\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(dataframe_[features_],dataframe_[target_],test_size=0.2, random_state=77, stratify=dataframe_[target_])\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be0ca120",
      "metadata": {
        "id": "be0ca120"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cl in requirement_classes:\n",
        "  print(cl, '\\n', class_dict[cl].value_counts(),'\\n\\n')"
      ],
      "metadata": {
        "id": "yHRuhZ5cP2eZ"
      },
      "id": "yHRuhZ5cP2eZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be896925",
      "metadata": {
        "id": "be896925"
      },
      "outputs": [],
      "source": [
        "X_class_train = dict()\n",
        "X_class_test = dict()\n",
        "y_class_train = dict()\n",
        "y_class_test = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    X_class_train[cl],X_class_test[cl],y_class_train[cl],y_class_test[cl]=split_train_test(dataframe_=bow_data, target_=cl, features_=class_dict[cl].index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f991ee0",
      "metadata": {
        "id": "6f991ee0"
      },
      "source": [
        "# Нормализация"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de36557",
      "metadata": {
        "id": "0de36557"
      },
      "source": [
        "Нормализация обучающей выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1438267b",
      "metadata": {
        "id": "1438267b"
      },
      "outputs": [],
      "source": [
        "scaler = dict()\n",
        "scaler_params = dict()\n",
        "for cl in requirement_classes:\n",
        "    scaler[cl] = MinMaxScaler()\n",
        "    X_class_train[cl] = scaler[cl].fit_transform(X_class_train[cl])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfebb4d",
      "metadata": {
        "id": "8bfebb4d"
      },
      "source": [
        "Нормализация тестовой выборки на сонове парамтеров обучающей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3800a77d",
      "metadata": {
        "id": "3800a77d"
      },
      "outputs": [],
      "source": [
        "scaler_ = MinMaxScaler()\n",
        "for cl in requirement_classes:\n",
        "#    scaler[cl].get_params()\n",
        "    X_class_test[cl]=scaler[cl].transform(X_class_test[cl])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628cc6e1",
      "metadata": {
        "id": "628cc6e1"
      },
      "source": [
        "# Обучение модели на базе Bag Of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b860011a",
      "metadata": {
        "id": "b860011a"
      },
      "source": [
        "Так как классификация будет проводиться по каждому из классов независимо, разбиение на тестовую и обучающую выборки будет проводиться для каждого из классов отдельно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce7acfb",
      "metadata": {
        "id": "1ce7acfb"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_class_lr = dict()\n",
        "model_lr_ = LogisticRegression() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_lr_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_lr[requirement_classes[cl]] = model_lr_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f0ee0e",
      "metadata": {
        "id": "55f0ee0e"
      },
      "source": [
        "Наивный Байес"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f510739",
      "metadata": {
        "id": "0f510739"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model_nb_ = GaussianNB() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "model_class_nb = dict()\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_nb_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_nb[requirement_classes[cl]] = model_nb_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbaa487a",
      "metadata": {
        "id": "bbaa487a"
      },
      "source": [
        "k-ближайших соседей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3077ed1",
      "metadata": {
        "id": "d3077ed1"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "model_knn_ = KNeighborsClassifier() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "model_class_knn = dict()\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_knn_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_knn[requirement_classes[cl]] = model_knn_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88512971",
      "metadata": {
        "id": "88512971"
      },
      "source": [
        "Метод решающих деревьев"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d253ec8a",
      "metadata": {
        "id": "d253ec8a"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_dtc_ = DecisionTreeClassifier() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "model_class_dtc = dict()\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_dtc_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_dtc[requirement_classes[cl]] = model_dtc_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2397622b",
      "metadata": {
        "id": "2397622b"
      },
      "source": [
        "Метод поддерживающих векторов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbdf11d",
      "metadata": {
        "id": "ffbdf11d"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "model_svc_ = SVC() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "model_class_svc = dict()\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_svc_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_svc[requirement_classes[cl]] = model_svc_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1040535",
      "metadata": {
        "id": "f1040535"
      },
      "source": [
        "Случайный лес"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56d1ac2",
      "metadata": {
        "id": "f56d1ac2"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_rf_ = RandomForestClassifier() # Используем модель с параметрами по умолчанию\n",
        "\n",
        "model_class_rf = dict()\n",
        "for cl in range(len(requirement_classes)):\n",
        "    model_rf_.fit(X_class_train[requirement_classes[cl]], y_class_train[requirement_classes[cl]])\n",
        "    model_class_rf[requirement_classes[cl]] = model_rf_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_models = { 'nb' : model_class_nb,\n",
        "                     'knn' : model_class_knn,\n",
        "                     'lr' : model_class_lr,\n",
        "                     'dtc' : model_class_dtc,\n",
        "                     'svc' : model_class_svc,\n",
        "                     'rf' : model_class_rf\n",
        "                     }"
      ],
      "metadata": {
        "id": "2C1k71W1ABIT"
      },
      "id": "2C1k71W1ABIT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e68d01ad",
      "metadata": {
        "id": "e68d01ad"
      },
      "source": [
        "## Расчет accuracy and confusion matrix метрик для каждого класса и метода"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb11eedd",
      "metadata": {
        "id": "eb11eedd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed68a200",
      "metadata": {
        "cellView": "form",
        "id": "ed68a200"
      },
      "outputs": [],
      "source": [
        "#@title linear regression\n",
        "\n",
        "lr_accuracy_test = dict()\n",
        "lr_accuracy_train = dict()\n",
        "lr_train_confusion_matrix = dict()\n",
        "lr_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    train_predict = model_class_lr[cl].predict(X_class_train[cl])\n",
        "    test_predict = model_class_lr[cl].predict(X_class_test[cl])\n",
        "\n",
        "    lr_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "    lr_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "\n",
        "    lr_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "    lr_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d925cc8",
      "metadata": {
        "cellView": "form",
        "id": "6d925cc8"
      },
      "outputs": [],
      "source": [
        "#@title naive bias\n",
        "\n",
        "nb_accuracy_test = dict()\n",
        "nb_accuracy_train = dict()\n",
        "nb_train_confusion_matrix = dict()\n",
        "nb_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    train_predict = model_class_nb[cl].predict(X_class_train[cl])\n",
        "    test_predict = model_class_nb[cl].predict(X_class_test[cl])\n",
        "\n",
        "    nb_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "    nb_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "\n",
        "    nb_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "    nb_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d47c9b",
      "metadata": {
        "cellView": "form",
        "id": "b7d47c9b"
      },
      "outputs": [],
      "source": [
        "#@title k nearest neighbours\n",
        "\n",
        "knn_accuracy_test = dict()\n",
        "knn_accuracy_train = dict()\n",
        "\n",
        "knn_train_confusion_matrix = dict()\n",
        "knn_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    train_predict = model_class_knn[cl].predict(X_class_train[cl])\n",
        "    test_predict = model_class_knn[cl].predict(X_class_test[cl])\n",
        "\n",
        "    knn_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "    knn_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "    \n",
        "    knn_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "    knn_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db2604f",
      "metadata": {
        "cellView": "form",
        "id": "7db2604f"
      },
      "outputs": [],
      "source": [
        "#@title decision trees\n",
        "\n",
        "\n",
        "dtc_accuracy_test = dict()\n",
        "dtc_accuracy_train = dict()\n",
        "dtc_train_confusion_matrix = dict()\n",
        "dtc_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    train_predict = model_class_dtc[cl].predict(X_class_train[cl])\n",
        "    test_predict = model_class_dtc[cl].predict(X_class_test[cl])\n",
        "\n",
        "    dtc_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "    dtc_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "\n",
        "    dtc_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "    dtc_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22147c89",
      "metadata": {
        "cellView": "form",
        "id": "22147c89"
      },
      "outputs": [],
      "source": [
        "#@title support vector machine\n",
        "\n",
        "svc_accuracy_test = dict()\n",
        "svc_accuracy_train = dict()\n",
        "svc_train_confusion_matrix = dict()\n",
        "svc_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    train_predict = model_class_svc[cl].predict(X_class_train[cl])\n",
        "    test_predict = model_class_svc[cl].predict(X_class_test[cl])\n",
        "\n",
        "    svc_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "    svc_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "    \n",
        "    svc_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "    svc_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9400a01",
      "metadata": {
        "id": "c9400a01"
      },
      "outputs": [],
      "source": [
        "#@title random forest\n",
        "\n",
        "rf_accuracy_test = dict()\n",
        "rf_accuracy_train = dict()\n",
        "rf_train_confusion_matrix = dict()\n",
        "rf_test_confusion_matrix = dict()\n",
        "\n",
        "for cl in requirement_classes:\n",
        "  train_predict = model_class_rf[cl].predict(X_class_train[cl])\n",
        "  test_predict = model_class_rf[cl].predict(X_class_test[cl])\n",
        "  \n",
        "  rf_accuracy_train[cl] = accuracy_score(y_class_train[cl], train_predict)\n",
        "  rf_accuracy_test[cl] = accuracy_score(y_class_test[cl], test_predict)\n",
        "  \n",
        "  rf_train_confusion_matrix[cl] = metrics.confusion_matrix(y_class_train[cl], train_predict)\n",
        "  rf_test_confusion_matrix[cl] = metrics.confusion_matrix(y_class_test[cl], test_predict)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e07980b",
      "metadata": {
        "id": "9e07980b"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b23ca6",
      "metadata": {
        "cellView": "form",
        "id": "b3b23ca6"
      },
      "outputs": [],
      "source": [
        "#@title variables declaration\n",
        "classifiers = list({'lr', 'nb', 'knn', 'dtc', 'svc', 'rf'})\n",
        "mean_train_accuracy = dict()\n",
        "mean_test_accuracy = dict()\n",
        "\n",
        "for i in classifiers:\n",
        "    mean_train_accuracy[i] = 0.0\n",
        "    mean_test_accuracy[i] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confusion matrix function definition\n",
        "cf = dict()\n",
        "def confusion_matrix_by_classifier (classifier, confusion_matrix):\n",
        "  cf[classifier] = 0\n",
        "  for cl in requirement_classes:\n",
        "    cf[classifier] += confusion_matrix[cl]\n",
        "  print('confusion matrix for ', classifier, ' classifier:\\n', cf[classifier], '\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TUttCzUnu1yb"
      },
      "id": "TUttCzUnu1yb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Confusion Matrixes for different classifiers\n",
        "lr_confusionMatrix = confusion_matrix_by_classifier('lr', lr_test_confusion_matrix)\n",
        "nb_confusionMatrix = confusion_matrix_by_classifier('nb', nb_test_confusion_matrix)\n",
        "svc_confusionMatrix = confusion_matrix_by_classifier('svc', svc_test_confusion_matrix)\n",
        "rf_confusionMatrix = confusion_matrix_by_classifier('rf', rf_test_confusion_matrix)\n",
        "dtc_confusion_matrix = confusion_matrix_by_classifier('dtc', dtc_test_confusion_matrix)\n",
        "knn_confusion_matrix = confusion_matrix_by_classifier('knn', knn_test_confusion_matrix)"
      ],
      "metadata": {
        "id": "ooXr2Yvk1ZVl"
      },
      "id": "ooXr2Yvk1ZVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mean Accuracy"
      ],
      "metadata": {
        "id": "jILiEROD5085"
      },
      "id": "jILiEROD5085"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4214e92f",
      "metadata": {
        "id": "4214e92f"
      },
      "outputs": [],
      "source": [
        "#@title logistic regression\n",
        "print('Логистическая регрессия')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',lr_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', lr_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['lr'] += lr_accuracy_train[cl]\n",
        "    mean_test_accuracy['lr'] += lr_accuracy_test[cl]\n",
        "\n",
        "mean_train_accuracy['lr'] = mean_train_accuracy['lr']/len(requirement_classes)\n",
        "mean_test_accuracy['lr'] = mean_test_accuracy['lr']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['lr'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['lr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76d6ae5",
      "metadata": {
        "cellView": "form",
        "id": "c76d6ae5"
      },
      "outputs": [],
      "source": [
        "#@title naive bias\n",
        "\n",
        "print('Наивный Байес')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',nb_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', nb_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['nb'] += nb_accuracy_train[cl]\n",
        "    mean_test_accuracy['nb'] += nb_accuracy_test[cl]\n",
        "mean_train_accuracy['nb'] = mean_train_accuracy['nb']/len(requirement_classes)\n",
        "mean_test_accuracy['nb'] = mean_test_accuracy['nb']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['nb'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['nb'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074fbea3",
      "metadata": {
        "cellView": "form",
        "id": "074fbea3"
      },
      "outputs": [],
      "source": [
        "#@title KNN\n",
        "\n",
        "print('k-ближайших соседей')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',knn_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', knn_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['knn'] += knn_accuracy_train[cl]\n",
        "    mean_test_accuracy['knn'] += knn_accuracy_test[cl]\n",
        "mean_train_accuracy['knn'] = mean_train_accuracy['knn']/len(requirement_classes)\n",
        "mean_test_accuracy['knn'] = mean_test_accuracy['knn']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['knn'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['knn'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e583b65",
      "metadata": {
        "cellView": "form",
        "id": "9e583b65"
      },
      "outputs": [],
      "source": [
        "#@title decision trees\n",
        "\n",
        "print('Метод решающих деревьев')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',dtc_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', dtc_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['dtc'] += dtc_accuracy_train[cl]\n",
        "    mean_test_accuracy['dtc'] += dtc_accuracy_test[cl]\n",
        "mean_train_accuracy['dtc'] = mean_train_accuracy['dtc']/len(requirement_classes)\n",
        "mean_test_accuracy['dtc'] = mean_test_accuracy['dtc']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['dtc'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['dtc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1f22d7",
      "metadata": {
        "cellView": "form",
        "id": "8c1f22d7"
      },
      "outputs": [],
      "source": [
        "#@title support vector machine)\n",
        "\n",
        "print('Метод опорных векторов')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',svc_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', svc_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['svc'] += svc_accuracy_train[cl]\n",
        "    mean_test_accuracy['svc'] += svc_accuracy_test[cl]\n",
        "mean_train_accuracy['svc'] = mean_train_accuracy['svc']/len(requirement_classes)\n",
        "mean_test_accuracy['svc'] = mean_test_accuracy['svc']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['svc'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['svc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21def33",
      "metadata": {
        "cellView": "form",
        "id": "b21def33"
      },
      "outputs": [],
      "source": [
        "#@title random forest\n",
        "\n",
        "print('Случайный лес')\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    '''\n",
        "    print(cl)\n",
        "    print ('Точность на обучающей выборке: ',rf_accuracy_train[cl])\n",
        "    print('Точность на тестовой выборке: ', rf_accuracy_test[cl], end='\\n\\n')\n",
        "    '''\n",
        "    mean_train_accuracy['rf'] += rf_accuracy_train[cl]\n",
        "    mean_test_accuracy['rf'] += rf_accuracy_test[cl]\n",
        "mean_train_accuracy['rf'] = mean_train_accuracy['rf']/len(requirement_classes)\n",
        "mean_test_accuracy['rf'] = mean_test_accuracy['rf']/len(requirement_classes)\n",
        "\n",
        "print ('mean train accuracy: ', mean_train_accuracy['rf'])\n",
        "print ('mean test accuracy: ', mean_test_accuracy['rf'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Precision"
      ],
      "metadata": {
        "id": "ePFey1G-81d5"
      },
      "id": "ePFey1G-81d5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dictionary to store Precision metric for each classifier\n",
        "precision_metrics = {}\n",
        "\n",
        "# Iterate over each classifier and its predictions\n",
        "for classifier_name in classifiers:\n",
        "    precision_metrics[classifier_name] = 0\n",
        "    for cl in requirement_classes:\n",
        "      test_predict = classifier_models[classifier_name][cl].predict(X_class_test[cl])\n",
        "      \n",
        "      precision_metrics[classifier_name] += metrics.precision_score(y_class_test[cl], test_predict)\n",
        "    \n",
        "    precision_metrics[classifier_name] = precision_metrics[classifier_name]/len(requirement_classes)\n",
        "    print(\"precision for \", classifier_name, '\\n', precision_metrics[classifier_name], '\\n')"
      ],
      "metadata": {
        "id": "digXrOx79DTX"
      },
      "id": "digXrOx79DTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recall"
      ],
      "metadata": {
        "id": "yF-YfnR-89JR"
      },
      "id": "yF-YfnR-89JR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dictionary to store Recall metric for each classifier\n",
        "recall_metrics = {}\n",
        "\n",
        "# Iterate over each classifier and its predictions\n",
        "for classifier_name in classifiers:\n",
        "    recall_metrics[classifier_name] = 0\n",
        "    for cl in requirement_classes:\n",
        "        test_predict = classifier_models[classifier_name][cl].predict(X_class_test[cl])\n",
        "        recall_metrics[classifier_name] += metrics.recall_score(y_class_test[cl], test_predict)\n",
        "\n",
        "    recall_metrics[classifier_name] = recall_metrics[classifier_name] / len(requirement_classes)\n",
        "    print(\"Recall for\", classifier_name, \":\\n\", recall_metrics[classifier_name], \"\\n\")\n"
      ],
      "metadata": {
        "id": "t_AJjYsG9Dq1"
      },
      "id": "t_AJjYsG9Dq1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##F1"
      ],
      "metadata": {
        "id": "hPHuKjzz884G"
      },
      "id": "hPHuKjzz884G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dictionary to store F1 metric for each classifier\n",
        "f1_metrics = {}\n",
        "\n",
        "# Iterate over each classifier and its predictions\n",
        "for classifier_name in classifiers:\n",
        "    f1_metrics[classifier_name] = 0\n",
        "    for cl in requirement_classes:\n",
        "        test_predict = classifier_models[classifier_name][cl].predict(X_class_test[cl])\n",
        "        f1_metrics[classifier_name] += metrics.f1_score(y_class_test[cl], test_predict)\n",
        "    \n",
        "    f1_metrics[classifier_name] = f1_metrics[classifier_name] / len(requirement_classes)\n",
        "    print(\"F1 metric for\", classifier_name, \":\", f1_metrics[classifier_name])\n"
      ],
      "metadata": {
        "id": "RddpH91w9EMD"
      },
      "id": "RddpH91w9EMD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2205fdc",
      "metadata": {
        "id": "d2205fdc"
      },
      "source": [
        "## Визуализация результатовб полученных на основе BOW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2efa515",
      "metadata": {
        "id": "f2efa515"
      },
      "source": [
        "методом линейной регрессии получилась следующая точность классификации для каждого класса:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d928a6b4",
      "metadata": {
        "scrolled": true,
        "id": "d928a6b4"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(requirement_classes,lr_accuracy_test.values(), width=0.6)\n",
        "plt.title('Точность линейной регрессионной классификации для каждого класса требований')\n",
        "a = plt.xticks(rotation=90)\n",
        "plt.axhline(0.8,c='r',linestyle='--',label='0.8')\n",
        "plt.axhline(0.85,c='w',linestyle='--',label='0.85')\n",
        "plt.axhline(0.9,c='black',linestyle='--',label='0.9')\n",
        "plt.ylim(0.7, 1.05) # adjust y axis limits\n",
        "plt.legend()\n",
        "plt.xlabel('Requirement class')\n",
        "plt.ylabel('Accuracy score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf81b442",
      "metadata": {
        "id": "bf81b442"
      },
      "outputs": [],
      "source": [
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers,mean_test_accuracy.values(),width=0.6, color={'red','green','blue','purple', 'skyblue','darkblue'})\n",
        "plt.title('Methods Accuracy')\n",
        "plt.axhline(0.8,c='r',linestyle='--',label='0.8')\n",
        "plt.axhline(0.85,c='w',linestyle='--',label='0.85')\n",
        "plt.axhline(0.9,c='black',linestyle='--',label='0.9')\n",
        "plt.legend()\n",
        "plt.autoscale()\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Accuracy score')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, f1_metrics.values(), width=0.6, color={'red','green','blue','purple', 'skyblue','darkblue'})\n",
        "plt.title('Methods F1 Score')\n",
        "plt.legend()\n",
        "plt.autoscale()\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_aRgL9XvFpK0"
      },
      "id": "_aRgL9XvFpK0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91aba4a",
      "metadata": {
        "id": "e91aba4a"
      },
      "outputs": [],
      "source": [
        "print ('method', '\\t', 'train accuracy', '\\t', 'test accuracy', '\\n')\n",
        "for i in classifiers:\n",
        "    print (i, '\\t', mean_train_accuracy[i], '\\t', mean_test_accuracy[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CountVectorizer model\n",
        "\n"
      ],
      "metadata": {
        "id": "3tTwRO4lWrkA"
      },
      "id": "3tTwRO4lWrkA"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title graph visualisation function for metrics\n",
        "\n",
        "def plot_scores_f1_accuracy(requirement_classes, f1_score, scores, number_of_requirements_per_class):\n",
        "    # Sort the F1-scores and number of requirements based on the F1-scores in descending order\n",
        "    sorted_indices = np.argsort(list(f1_score.values()))[::-1]\n",
        "    sorted_f1_scores = [list(f1_score.values())[i] for i in sorted_indices]\n",
        "    sorted_class_requirements = [number_of_requirements_per_class[i] for i in sorted_indices]\n",
        "    sorted_requirement_classes = [requirement_classes[i] for i in sorted_indices]\n",
        "    sorted_accuracy_scores = [scores[cl] for cl in sorted_requirement_classes]\n",
        "\n",
        "    # Normalize the F1-scores and accuracy scores between 0 and 1\n",
        "\n",
        "    normalized_f1_scores = [score for score in sorted_f1_scores]\n",
        "    normalized_accuracy_scores = [score for score in sorted_accuracy_scores]\n",
        "\n",
        "    # Set the width of the bars\n",
        "    bar_width = 0.4\n",
        "\n",
        "    # Set the x positions for the bars\n",
        "    x = np.arange(len(sorted_requirement_classes))\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Plot the F1-scores\n",
        "    ax1.bar(x, normalized_f1_scores, width=bar_width, label='F1-score', color='blue')\n",
        "\n",
        "    # Set the y-axis labels for F1-scores\n",
        "    ax1.set_ylabel('F1-score')\n",
        "    ax1.legend(loc='upper center')\n",
        "\n",
        "    # Create a secondary y-axis for the number of requirements\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(x, sorted_class_requirements, color='orange', linestyle='', marker='o', label='Number of Requirements')\n",
        "    #ax2.set_ylabel('Number of Requirements')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.yaxis.set_ticks([])  # Remove y-axis ticks for number of requirements\n",
        "\n",
        "    # Add the accuracy score as a separate bar chart\n",
        "    ax3 = ax1.twinx()\n",
        "    ax3.bar(x + bar_width, normalized_accuracy_scores, width=bar_width, label='Accuracy', color='green', alpha=0.5)\n",
        "    ax3.set_ylabel('Accuracy')\n",
        "    ax3.legend(loc='center right')\n",
        "\n",
        "    # Set the chart title\n",
        "    ax1.set_title('F1-score, Accuracy, and Number of Requirements for each Requirement Class')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(sorted_requirement_classes, rotation=90)\n",
        "\n",
        "    pad = 1.2  #@param {type:\"slider\", min:1.00, max:2.00, step:0.1}\n",
        "    plt.tight_layout(pad=pad)\n",
        "    # Display the chart\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RBCzVsG_eEwZ"
      },
      "id": "RBCzVsG_eEwZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title train test split with the help of function split_train_test\n",
        "\n",
        "X_class_train = dict()\n",
        "X_class_test = dict()\n",
        "y_class_train = dict()\n",
        "y_class_test = dict()\n",
        "\n",
        "class_requirements = [len(data[data[cl] == 1]) for cl in requirement_classes]\n",
        "\n",
        "for cl in requirement_classes:\n",
        "    X_class_train[cl],X_class_test[cl],y_class_train[cl],y_class_test[cl]=split_train_test(dataframe_=data, target_=cl, features_='lemmatized_requirement')"
      ],
      "metadata": {
        "id": "4PBsrEISXAlW",
        "cellView": "form"
      },
      "id": "4PBsrEISXAlW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize and transform CountVectorizers for each classification task\n",
        "# Initialize a CountVectorizer object: count_vectorizer\n",
        "count_train = dict()\n",
        "count_test = dict()\n",
        "count_vectorizers = dict()\n",
        "\n",
        "\n",
        "for cl in requirement_classes:\n",
        "\n",
        "  count_vectorizers[cl] = CountVectorizer(stop_words=\"english\")\n",
        "  \n",
        "  # Transform the training data using only the 'text' column values: count_train \n",
        "  count_train[cl] = count_vectorizers[cl].fit_transform(X_class_train[cl])\n",
        "\n",
        "  # Transform the test data using only the 'text' column values: count_test \n",
        "  count_test[cl] = count_vectorizers[cl].transform(X_class_test[cl])"
      ],
      "metadata": {
        "id": "6IgQIC9ZWqdG",
        "cellView": "form"
      },
      "id": "6IgQIC9ZWqdG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Naive Bias classifier"
      ],
      "metadata": {
        "id": "-_z6iQR0ulft"
      },
      "id": "-_z6iQR0ulft"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store naive bias classifiers\n",
        "nb_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Multinomial Naive Bayes classifier for the current task\n",
        "    nb_classifier = MultinomialNB()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    nb_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    nb_classifiers[cl] = nb_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "nb_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "nb_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "nb_precision = {}\n",
        "nb_recall = {}\n",
        "nb_f1_score = {}\n",
        "nb_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    nb_pred[cl] = nb_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    nb_scores[cl] = metrics.accuracy_score(y_class_test[cl], nb_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", nb_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    nb_cms[cl] = metrics.confusion_matrix(y_class_test[cl], nb_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", nb_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    nb_precision[cl] = metrics.precision_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_recall[cl] = metrics.recall_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_f1_score[cl] = metrics.f1_score(y_class_test[cl], nb_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')"
      ],
      "metadata": {
        "id": "dJwnCuBydzIb"
      },
      "id": "dJwnCuBydzIb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, nb_f1_score, nb_scores, class_requirements)"
      ],
      "metadata": {
        "id": "8WN_36w_ewo-"
      },
      "id": "8WN_36w_ewo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#@title graph naive bias F1-score and Number of Requirements for each Requirement Class\n",
        "\n",
        "# Sort the F1-scores and number of requirements based on the F1-scores in descending order\n",
        "sorted_indices = np.argsort(list(nb_f1_score.values()))[::-1]\n",
        "sorted_f1_scores = [list(nb_f1_score.values())[i] for i in sorted_indices]\n",
        "sorted_class_requirements = [class_requirements[i] for i in sorted_indices]\n",
        "sorted_requirement_classes = [requirement_classes[i] for i in sorted_indices]\n",
        "sorted_accuracy_scores = [nb_scores[cl] for cl in sorted_requirement_classes]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.4\n",
        "\n",
        "# Set the x positions for the bars\n",
        "x = np.arange(len(sorted_requirement_classes))\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot the F1-scores\n",
        "ax1.bar(x, sorted_f1_scores, width=bar_width, label='F1-score', color='blue')\n",
        "\n",
        "# Set the y-axis labels for F1-scores\n",
        "ax1.set_ylabel('F1-score')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Create a secondary y-axis for the number of requirements\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(x, sorted_class_requirements, color='orange', linestyle='', marker='o', label='Number of Requirements')\n",
        "ax2.set_ylabel('Number of Requirements')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Set the x-axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(sorted_requirement_classes, rotation=90)\n",
        "\n",
        "# Add the accuracy score as a separate bar chart\n",
        "ax3 = ax1.twinx()\n",
        "ax3.bar(x + bar_width, sorted_accuracy_scores, width=bar_width, label='Accuracy', color='green', alpha=0.5)\n",
        "ax3.set_ylabel('Accuracy')\n",
        "ax3.legend(loc='upper center')\n",
        "\n",
        "# Set the chart title\n",
        "ax1.set_title('F1-score, Accuracy, and Number of Requirements for each Requirement Class')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()'''\n"
      ],
      "metadata": {
        "id": "wCdd6V6Nlc7e",
        "cellView": "form"
      },
      "id": "wCdd6V6Nlc7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Logistic regression classifier"
      ],
      "metadata": {
        "id": "TGo3LRP8uvs4"
      },
      "id": "TGo3LRP8uvs4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store linear regression classifiers\n",
        "lr_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a linear regression classifier for the current task\n",
        "    lr_classifier = LogisticRegression()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    lr_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    lr_classifiers[cl] = lr_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "lr_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "lr_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "lr_precision = {}\n",
        "lr_recall = {}\n",
        "lr_f1_score = {}\n",
        "lr_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    lr_pred[cl] = lr_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    lr_scores[cl] = metrics.accuracy_score(y_class_test[cl], lr_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", lr_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    lr_cms[cl] = metrics.confusion_matrix(y_class_test[cl], lr_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", lr_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    lr_precision[cl] = metrics.precision_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_recall[cl] = metrics.recall_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_f1_score[cl] = metrics.f1_score(y_class_test[cl], lr_pred[cl])\n",
        "    \n",
        "    #print(\"Precision for\", cl, \":\", lr_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", lr_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", lr_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "60wjRRouuuIP"
      },
      "id": "60wjRRouuuIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, lr_f1_score, lr_scores, class_requirements)"
      ],
      "metadata": {
        "id": "3MQsJNZau2ER"
      },
      "id": "3MQsJNZau2ER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest"
      ],
      "metadata": {
        "id": "-qdffBzBvxPF"
      },
      "id": "-qdffBzBvxPF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Random Forest classifiers\n",
        "rf_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Random Forest classifier for the current task\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    rf_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    rf_classifiers[cl] = rf_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "rf_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "rf_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "rf_precision = {}\n",
        "rf_recall = {}\n",
        "rf_f1_score = {}\n",
        "rf_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    rf_pred[cl] = rf_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    rf_scores[cl] = metrics.accuracy_score(y_class_test[cl], rf_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", rf_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    rf_cms[cl] = metrics.confusion_matrix(y_class_test[cl], rf_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", rf_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    rf_precision[cl] = metrics.precision_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_recall[cl] = metrics.recall_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_f1_score[cl] = metrics.f1_score(y_class_test[cl], rf_pred[cl])\n",
        "\n",
        "    # Print precision, recall, and F1-score for the current task\n",
        "    #print(\"Precision for\", cl, \":\", rf_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", rf_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", rf_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "UcWTOtW3vzWQ"
      },
      "id": "UcWTOtW3vzWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CountVectorizer F1-score and Number of Requirements for each Requirement Class\n",
        "\n",
        "plot_scores_f1_accuracy(requirement_classes, rf_f1_score, rf_scores, class_requirements)\n"
      ],
      "metadata": {
        "id": "WZOmPCxfv2Qq",
        "cellView": "form"
      },
      "id": "WZOmPCxfv2Qq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN"
      ],
      "metadata": {
        "id": "8wtTtc8lwi10"
      },
      "id": "8wtTtc8lwi10"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store KNN classifiers\n",
        "knn_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a KNN classifier for the current task\n",
        "    knn_classifier = KNeighborsClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    knn_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    knn_classifiers[cl] = knn_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "knn_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "knn_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "knn_precision = {}\n",
        "knn_recall = {}\n",
        "knn_f1_score = {}\n",
        "knn_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    knn_pred[cl] = knn_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    knn_scores[cl] = metrics.accuracy_score(y_class_test[cl], knn_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", knn_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    knn_cms[cl] = metrics.confusion_matrix(y_class_test[cl], knn_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", knn_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    knn_precision[cl] = metrics.precision_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_recall[cl] = metrics.recall_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_f1_score[cl] = metrics.f1_score(y_class_test[cl], knn_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "_LEcef_fwj6f"
      },
      "id": "_LEcef_fwj6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, knn_f1_score, knn_scores, class_requirements)"
      ],
      "metadata": {
        "id": "gNpLDZ9TwkhP"
      },
      "id": "gNpLDZ9TwkhP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Decision Tree"
      ],
      "metadata": {
        "id": "w0vsVSsEwv1x"
      },
      "id": "w0vsVSsEwv1x"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Decision Tree classifiers\n",
        "dtc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Decision Tree classifier for the current task\n",
        "    dtc_classifier = DecisionTreeClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    dtc_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    dtc_classifiers[cl] = dtc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "dtc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "dtc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "dtc_precision = {}\n",
        "dtc_recall = {}\n",
        "dtc_f1_score = {}\n",
        "dtc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    dtc_pred[cl] = dtc_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    dtc_scores[cl] = metrics.accuracy_score(y_class_test[cl], dtc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", dtc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    dtc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], dtc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", dtc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    dtc_precision[cl] = metrics.precision_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_recall[cl] = metrics.recall_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_f1_score[cl] = metrics.f1_score(y_class_test[cl], dtc_pred[cl])"
      ],
      "metadata": {
        "id": "qW2mK0_kwu9-"
      },
      "id": "qW2mK0_kwu9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title graph F1-score and Number of Requirements for each Requirement Class\n",
        "plot_scores_f1_accuracy(requirement_classes, dtc_f1_score, dtc_scores, class_requirements)\n",
        "'''\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# Get the number of requirements for each class\n",
        "dtc_sorted_f1_score = dict(sorted(dtc_f1_score.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "bar_width = 0.4\n",
        "\n",
        "# Plot the F1-score\n",
        "ax1.bar(dtc_sorted_f1_score.keys(), dtc_sorted_f1_score.values(), width=bar_width, label='F1-score', color='blue')\n",
        "ax1.set_ylabel('F1-score')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "plt.title('F1-score and Number of Requirements for each Requirement Class')\n",
        "plt.xlabel('Requirement Class')\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Create a secondary y-axis for number of requirements\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(requirement_classes, class_requirements, color='orange', linestyle='', marker='o', label='Number of Requirements')\n",
        "ax2.set_ylabel('Number of Requirements')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "3fCoGcOjxK7f"
      },
      "id": "3fCoGcOjxK7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SVC"
      ],
      "metadata": {
        "id": "DttckLCxxZkq"
      },
      "id": "DttckLCxxZkq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize a dictionary to store SVC classifiers\n",
        "svc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate an SVC classifier for the current task\n",
        "    svc_classifier = SVC()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    svc_classifier.fit(count_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    svc_classifiers[cl] = svc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "svc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "svc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "svc_precision = {}\n",
        "svc_recall = {}\n",
        "svc_f1_score = {}\n",
        "svc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Predict the tags for the test data of the current task\n",
        "    svc_pred[cl] = svc_classifiers[cl].predict(count_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    svc_scores[cl] = metrics.accuracy_score(y_class_test[cl], svc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", svc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    svc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], svc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", svc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    svc_precision[cl] = metrics.precision_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_recall[cl] = metrics.recall_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_f1_score[cl] = metrics.f1_score(y_class_test[cl], svc_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "odxkNKfdxZWD"
      },
      "id": "odxkNKfdxZWD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title graph F1-score and Accuracy Score for each Requirement Class\n",
        "plot_scores_f1_accuracy(requirement_classes, svc_f1_score, svc_scores, class_requirements)\n",
        "'''\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "# Get the number of requirements for each class\n",
        "svc_sorted_f1_score = dict(sorted(svc_f1_score.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "bar_width = 0.4\n",
        "\n",
        "# Compute the x-positions for the F1-score bars\n",
        "x_f1 = np.arange(len(svc_sorted_f1_score))\n",
        "\n",
        "# Plot the F1-score bars\n",
        "ax1.bar(x_f1, svc_sorted_f1_score.values(), width=bar_width, label='F1-score', color='blue')\n",
        "\n",
        "# Compute the x-positions for the accuracy bars\n",
        "x_acc = x_f1 + bar_width\n",
        "\n",
        "# Plot the accuracy bars\n",
        "ax1.bar(x_acc, svc_scores.values(), width=bar_width, label='Accuracy score', color='green', alpha=0.5)\n",
        "\n",
        "ax1.set_ylabel('Scores')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "plt.title('F1-score and Accuracy Score for each Requirement Class')\n",
        "plt.xlabel('Requirement Class')\n",
        "plt.xticks(ticks=(x_f1 + bar_width/2), labels=svc_sorted_f1_score.keys(), rotation=90)\n",
        "\n",
        "# Create a secondary y-axis for number of requirements\n",
        "#ax2 = ax1.twinx()\n",
        "#ax2.plot(svc_sorted_f1_score, class_requirements, color='orange', linestyle='', marker='o', label='Number of Requirements')\n",
        "#ax2.set_ylabel('Number of Requirements')\n",
        "#ax2.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rdcf4p-Mxzqo"
      },
      "id": "rdcf4p-Mxzqo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(list(nb_f1_score.values()))"
      ],
      "metadata": {
        "id": "R0RlOxD31d4v"
      },
      "id": "R0RlOxD31d4v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_f1_score"
      ],
      "metadata": {
        "id": "CRiIlc7526LQ"
      },
      "id": "CRiIlc7526LQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods comparison for CountVectorizer"
      ],
      "metadata": {
        "id": "0G2E4aRcAoOB"
      },
      "id": "0G2E4aRcAoOB"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title f1_scores\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, [np.mean(scores) for scores in f1_metrics], width=0.6, color=['red', 'green', 'blue', 'purple', 'skyblue', 'darkblue'])\n",
        "plt.title('Methods F1 Score')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YXcOwxgrxXPw",
        "cellView": "form"
      },
      "id": "YXcOwxgrxXPw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title accuracy scores\n",
        "# Create a list to store the F1-scores\n",
        "scores_ = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "scores_.append(list(nb_scores.values()))\n",
        "scores_.append(list(lr_scores.values()))\n",
        "scores_.append(list(knn_scores.values()))\n",
        "scores_.append(list(dtc_scores.values()))\n",
        "scores_.append(list(svc_scores.values()))\n",
        "scores_.append(list(rf_scores.values()))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, [np.mean(scores) for scores in scores_], width=0.6, color=['red', 'green', 'blue', 'purple', 'skyblue', 'darkblue'])\n",
        "plt.title('Methods Accuracy')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FOnyliZU5cGa",
        "cellView": "form"
      },
      "id": "FOnyliZU5cGa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Methods Performance Comparison for CountVectorizer\n",
        "# Create a list to store the F1-scores\n",
        "scores_ = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "scores_.append(list(nb_scores.values()))\n",
        "scores_.append(list(lr_scores.values()))\n",
        "scores_.append(list(knn_scores.values()))\n",
        "scores_.append(list(dtc_scores.values()))\n",
        "scores_.append(list(svc_scores.values()))\n",
        "scores_.append(list(rf_scores.values()))\n",
        "\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "bar_width = 0.35\n",
        "offset = np.arange(len(classifiers))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the accuracy scores\n",
        "plt.bar(offset - bar_width/2, [np.mean(scores) for scores in scores_], width=bar_width, label='Accuracy', color='blue')\n",
        "\n",
        "# Plot the F1-scores\n",
        "plt.bar(offset + bar_width/2, [np.mean(scores) for scores in f1_metrics], width=bar_width, label='F1-score', color='red')\n",
        "\n",
        "plt.title('Methods Performance Comparison')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(offset, classifiers)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xXWm3uZW6fRs"
      },
      "id": "xXWm3uZW6fRs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFIDF Vectorizer"
      ],
      "metadata": {
        "id": "lrdVxTHcxiud"
      },
      "id": "lrdVxTHcxiud"
    },
    {
      "cell_type": "code",
      "source": [
        "term_max_document_frequency = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "metadata": {
        "id": "ewOyBOcTDIDc"
      },
      "id": "ewOyBOcTDIDc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize and transform TfidfVectorizers for each classification task\n",
        "\n",
        "tfidf_train = dict()\n",
        "tfidf_test = dict()\n",
        "tfidf_vectorizers = dict() \n",
        "\n",
        "for cl in requirement_classes:\n",
        "\n",
        "  tfidf_vectorizers[cl] = TfidfVectorizer(stop_words=\"english\", max_df=term_max_document_frequency)\n",
        "  \n",
        "  # Transform the training data using only the 'text' column values: count_train \n",
        "  tfidf_train[cl] = tfidf_vectorizers[cl].fit_transform(X_class_train[cl])\n",
        "\n",
        "  # Transform the test data using only the 'text' column values: count_test \n",
        "  tfidf_test[cl] = tfidf_vectorizers[cl].transform(X_class_test[cl])"
      ],
      "metadata": {
        "id": "YWPSspsNBd6Y"
      },
      "id": "YWPSspsNBd6Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive bias"
      ],
      "metadata": {
        "id": "8L5OfOvEAnQO"
      },
      "id": "8L5OfOvEAnQO"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title naive bias classifier for TF-IDF Vectorizer\n",
        "\n",
        "# Initialize a dictionary to store naive bias classifiers\n",
        "nb_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Multinomial Naive Bayes classifier for the current task\n",
        "    nb_classifier = MultinomialNB()\n",
        "\n",
        "    # Fit the classifier to the training data for the current task\n",
        "    nb_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "\n",
        "    # Store the classifier in the dictionary\n",
        "    nb_classifiers[cl] = nb_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "nb_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "nb_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "nb_precision = {}\n",
        "nb_recall = {}\n",
        "nb_f1_score = {}\n",
        "nb_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    nb_pred[cl] = nb_classifiers[cl].predict(tfidf_test)\n",
        "\n",
        "    # Calculate the accuracy score for the current task\n",
        "    nb_scores[cl] = metrics.accuracy_score(y_class_test[cl], nb_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", nb_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    nb_cms[cl] = metrics.confusion_matrix(y_class_test[cl], nb_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", nb_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    nb_precision[cl] = metrics.precision_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_recall[cl] = metrics.recall_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_f1_score[cl] = metrics.f1_score(y_class_test[cl], nb_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", nb_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1fEwSa0uDX6T"
      },
      "id": "1fEwSa0uDX6T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graph for F1-score of naive bias classifier for TF-IDF Vectorizer\n",
        "\n",
        "plot_scores_f1_accuracy(requirement_classes, nb_f1_score, nb_scores, class_requirements)\n",
        "\n",
        "'''\n",
        "# Sort the F1-scores and number of requirements based on the F1-scores in descending order\n",
        "sorted_indices = np.argsort(list(nb_f1_score.values()))[::-1]\n",
        "sorted_f1_scores = [list(nb_f1_score.values())[i] for i in sorted_indices]\n",
        "sorted_class_requirements = [class_requirements[i] for i in sorted_indices]\n",
        "sorted_requirement_classes = [requirement_classes[i] for i in sorted_indices]\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.4\n",
        "\n",
        "# Set the x positions for the bars\n",
        "x = np.arange(len(requirement_classes))\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot the F1-scores\n",
        "ax1.bar(x, sorted_f1_scores, width=bar_width, label='F1-score', color='blue')\n",
        "\n",
        "# Set the y-axis labels for F1-scores\n",
        "ax1.set_ylabel('F1-score')\n",
        "ax1.legend(loc='upper left')\n",
        "\n",
        "# Create a secondary y-axis for the number of requirements\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(x, sorted_class_requirements, color='orange', linestyle='', marker='o', label='Number of Requirements')\n",
        "ax2.set_ylabel('Number of Requirements')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Set the x-axis labels\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(sorted_requirement_classes, rotation=90)\n",
        "\n",
        "# Set the chart title\n",
        "ax1.set_title('F1-score and Number of Requirements for each Requirement Class')\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UUQNmF6xVpjM"
      },
      "id": "UUQNmF6xVpjM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Graph for F1-score and Accuracy \n",
        "'''\n",
        "# Get the F1-scores and accuracy scores for each class\n",
        "f1_scores = list(nb_f1_score.values())\n",
        "accuracy_scores = list(nb_scores.values())\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.35\n",
        "\n",
        "# Set the x positions for the bars\n",
        "x = np.arange(len(requirement_classes))\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot the F1-scores\n",
        "ax.bar(x - bar_width/2, f1_scores, width=bar_width, label='F1-score', color='blue')\n",
        "\n",
        "# Plot the accuracy scores\n",
        "ax.bar(x + bar_width/2, accuracy_scores, width=bar_width, label='Accuracy', color='green')\n",
        "\n",
        "# Set the x-axis labels\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(requirement_classes, rotation=90)\n",
        "\n",
        "# Set the y-axis labels\n",
        "ax.set_ylabel('Score')\n",
        "\n",
        "# Set the chart title\n",
        "ax.set_title('F1-score and Accuracy for each Requirement Class')\n",
        "\n",
        "# Add a legend\n",
        "ax.legend()\n",
        "\n",
        "# Display the chart\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qKdW0y3LRqTc"
      },
      "id": "qKdW0y3LRqTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic regression"
      ],
      "metadata": {
        "id": "qQqIi8-03yJL"
      },
      "id": "qQqIi8-03yJL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store linear regression classifiers\n",
        "lr_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a linear regression classifier for the current task\n",
        "    lr_classifier = LogisticRegression()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    lr_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    lr_classifiers[cl] = lr_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "lr_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "lr_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "lr_precision = {}\n",
        "lr_recall = {}\n",
        "lr_f1_score = {}\n",
        "lr_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    \n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    lr_pred[cl] = lr_classifiers[cl].predict(tfidf_test)\n",
        "    \n",
        "    #lr_pred[cl] = lr_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    lr_scores[cl] = metrics.accuracy_score(y_class_test[cl], lr_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", lr_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    lr_cms[cl] = metrics.confusion_matrix(y_class_test[cl], lr_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", lr_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    lr_precision[cl] = metrics.precision_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_recall[cl] = metrics.recall_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_f1_score[cl] = metrics.f1_score(y_class_test[cl], lr_pred[cl])\n",
        "    \n",
        "    #print(\"Precision for\", cl, \":\", lr_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", lr_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", lr_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "SFtY1jJKKcjf"
      },
      "id": "SFtY1jJKKcjf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, lr_f1_score, lr_scores, class_requirements)"
      ],
      "metadata": {
        "id": "W6BQiQSc0Awr"
      },
      "id": "W6BQiQSc0Awr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random forest"
      ],
      "metadata": {
        "id": "Wx-aq9hE1ExK"
      },
      "id": "Wx-aq9hE1ExK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Random Forest classifiers\n",
        "rf_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Random Forest classifier for the current task\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    rf_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    rf_classifiers[cl] = rf_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "rf_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "rf_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "rf_precision = {}\n",
        "rf_recall = {}\n",
        "rf_f1_score = {}\n",
        "rf_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    rf_pred[cl] = rf_classifiers[cl].predict(tfidf_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #rf_pred[cl] = rf_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    rf_scores[cl] = metrics.accuracy_score(y_class_test[cl], rf_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", rf_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    rf_cms[cl] = metrics.confusion_matrix(y_class_test[cl], rf_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", rf_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    rf_precision[cl] = metrics.precision_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_recall[cl] = metrics.recall_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_f1_score[cl] = metrics.f1_score(y_class_test[cl], rf_pred[cl])\n",
        "\n",
        "    # Print precision, recall, and F1-score for the current task\n",
        "    #print(\"Precision for\", cl, \":\", rf_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", rf_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", rf_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "ZCQH6nk60CSh"
      },
      "id": "ZCQH6nk60CSh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, rf_f1_score, rf_scores, class_requirements)"
      ],
      "metadata": {
        "id": "DmOVA_KL1RHz"
      },
      "id": "DmOVA_KL1RHz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN"
      ],
      "metadata": {
        "id": "0OD8HfCH3pt4"
      },
      "id": "0OD8HfCH3pt4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store KNN classifiers\n",
        "knn_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a KNN classifier for the current task\n",
        "    knn_classifier = KNeighborsClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    knn_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    knn_classifiers[cl] = knn_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "knn_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "knn_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "knn_precision = {}\n",
        "knn_recall = {}\n",
        "knn_f1_score = {}\n",
        "knn_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    knn_pred[cl] = knn_classifiers[cl].predict(tfidf_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #knn_pred[cl] = knn_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    knn_scores[cl] = metrics.accuracy_score(y_class_test[cl], knn_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", knn_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    knn_cms[cl] = metrics.confusion_matrix(y_class_test[cl], knn_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", knn_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    knn_precision[cl] = metrics.precision_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_recall[cl] = metrics.recall_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_f1_score[cl] = metrics.f1_score(y_class_test[cl], knn_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "PAi4iEnh2XIB"
      },
      "id": "PAi4iEnh2XIB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, knn_f1_score, knn_scores, class_requirements)"
      ],
      "metadata": {
        "id": "pb5TSZX12t6X"
      },
      "id": "pb5TSZX12t6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision tree"
      ],
      "metadata": {
        "id": "9T9XadK13mt5"
      },
      "id": "9T9XadK13mt5"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a dictionary to store Decision Tree classifiers\n",
        "dtc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Decision Tree classifier for the current task\n",
        "    dtc_classifier = DecisionTreeClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    dtc_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    dtc_classifiers[cl] = dtc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "dtc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "dtc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "dtc_precision = {}\n",
        "dtc_recall = {}\n",
        "dtc_f1_score = {}\n",
        "dtc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    dtc_pred[cl] = dtc_classifiers[cl].predict(tfidf_test)\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #dtc_pred[cl] = dtc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    dtc_scores[cl] = metrics.accuracy_score(y_class_test[cl], dtc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", dtc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    dtc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], dtc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", dtc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    dtc_precision[cl] = metrics.precision_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_recall[cl] = metrics.recall_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_f1_score[cl] = metrics.f1_score(y_class_test[cl], dtc_pred[cl])"
      ],
      "metadata": {
        "id": "V5oTaq-33FJw"
      },
      "id": "V5oTaq-33FJw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, dtc_f1_score, dtc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "BcVfIedE3Rlq"
      },
      "id": "BcVfIedE3Rlq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVC"
      ],
      "metadata": {
        "id": "M_V1ccUl4EPf"
      },
      "id": "M_V1ccUl4EPf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize a dictionary to store SVC classifiers\n",
        "svc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate an SVC classifier for the current task\n",
        "    svc_classifier = SVC()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    svc_classifier.fit(tfidf_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    svc_classifiers[cl] = svc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "svc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "svc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "svc_precision = {}\n",
        "svc_recall = {}\n",
        "svc_f1_score = {}\n",
        "svc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    tfidf_test = tfidf_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    svc_pred[cl] = svc_classifiers[cl].predict(tfidf_test)\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #svc_pred[cl] = svc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    svc_scores[cl] = metrics.accuracy_score(y_class_test[cl], svc_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", svc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    svc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], svc_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", svc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    svc_precision[cl] = metrics.precision_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_recall[cl] = metrics.recall_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_f1_score[cl] = metrics.f1_score(y_class_test[cl], svc_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "9Srmfwhx4AN-"
      },
      "id": "9Srmfwhx4AN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, svc_f1_score, svc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "3BAsSgt74deW"
      },
      "id": "3BAsSgt74deW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods comparison for TfidfVectorizer"
      ],
      "metadata": {
        "id": "Bc71sYRM5P0u"
      },
      "id": "Bc71sYRM5P0u"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title f1_scores\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, [np.mean(scores) for scores in f1_metrics], width=0.6, color=['red', 'green', 'blue', 'purple', 'skyblue', 'darkblue'])\n",
        "plt.title('Methods F1 Score')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IeLMZVTo4sqz"
      },
      "id": "IeLMZVTo4sqz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Methods Performance Comparison for CountVectorizer\n",
        "# Create a list to store the F1-scores\n",
        "scores_ = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "scores_.append(list(nb_scores.values()))\n",
        "scores_.append(list(lr_scores.values()))\n",
        "scores_.append(list(knn_scores.values()))\n",
        "scores_.append(list(dtc_scores.values()))\n",
        "scores_.append(list(svc_scores.values()))\n",
        "scores_.append(list(rf_scores.values()))\n",
        "\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "bar_width = 0.35\n",
        "offset = np.arange(len(classifiers))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the accuracy scores\n",
        "plt.bar(offset - bar_width/2, [np.mean(scores) for scores in scores_], width=bar_width, label='Accuracy', color='blue')\n",
        "\n",
        "# Plot the F1-scores\n",
        "plt.bar(offset + bar_width/2, [np.mean(scores) for scores in f1_metrics], width=bar_width, label='F1-score', color='red')\n",
        "\n",
        "plt.title('Methods Performance Comparison')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(offset, classifiers)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HXUqjs384tGs"
      },
      "id": "HXUqjs384tGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hashing Vectorizer"
      ],
      "metadata": {
        "id": "ctVGnz7rAv9B"
      },
      "id": "ctVGnz7rAv9B"
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "\n",
        "tfidf_train = dict()\n",
        "tfidf_test = dict()\n",
        "tfidf_vectorizers = dict() \n",
        "\n",
        "for cl in requirement_classes:\n",
        "\n",
        "  tfidf_vectorizers[cl] = TfidfVectorizer(stop_words=\"english\", max_df=term_max_document_frequency)\n",
        "  \n",
        "  # Transform the training data using only the 'text' column values: count_train \n",
        "  tfidf_train[cl] = tfidf_vectorizers[cl].fit_transform(X_class_train[cl])\n",
        "\n",
        "  # Transform the test data using only the 'text' column values: count_test \n",
        "  tfidf_test[cl] = tfidf_vectorizers[cl].transform(X_class_test[cl])'''"
      ],
      "metadata": {
        "id": "GLYH5tIFBYdW"
      },
      "id": "GLYH5tIFBYdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hv_number_of_features = 2**10 #@param"
      ],
      "metadata": {
        "id": "4X4gC6DKBofd"
      },
      "id": "4X4gC6DKBofd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title initialize and transform HashingVectorizers for each classification task\n",
        "\n",
        "hv_train = dict()\n",
        "hv_test = dict()\n",
        "hv_vectorizers = dict() \n",
        "\n",
        "for cl in requirement_classes:\n",
        "\n",
        "    hv_vectorizers[cl] = HashingVectorizer(stop_words=\"english\", n_features= hv_number_of_features)\n",
        "\n",
        "    # Transform the training data using only the 'text' column values: hash_train\n",
        "    hv_train[cl] = hv_vectorizers[cl].fit_transform(X_class_train[cl])\n",
        "\n",
        "    # Transform the test data using only the 'text' column values: hash_test\n",
        "    hv_test[cl] = hv_vectorizers[cl].transform(X_class_test[cl])\n"
      ],
      "metadata": {
        "id": "C9aWvxkUA0oY"
      },
      "id": "C9aWvxkUA0oY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive bias"
      ],
      "metadata": {
        "id": "stuZwJxYDdUg"
      },
      "id": "stuZwJxYDdUg"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize a dictionary to store naive bias classifiers\n",
        "nb_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Bernoulli Naive Bayes classifier for the current task\n",
        "    nb_classifier = BernoulliNB() # AS MultinomialNB cannot work with negative values\n",
        "\n",
        "    # Fit the classifier to the training data for the current task\n",
        "    nb_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "\n",
        "    # Store the classifier in the dictionary\n",
        "    nb_classifiers[cl] = nb_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "nb_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "nb_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "nb_precision = {}\n",
        "nb_recall = {}\n",
        "nb_f1_score = {}\n",
        "nb_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    nb_pred[cl] = nb_classifiers[cl].predict(hv_test)\n",
        "\n",
        "    # Calculate the accuracy score for the current task\n",
        "    nb_scores[cl] = metrics.accuracy_score(y_class_test[cl], nb_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", nb_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    nb_cms[cl] = metrics.confusion_matrix(y_class_test[cl], nb_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", nb_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    nb_precision[cl] = metrics.precision_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_recall[cl] = metrics.recall_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_f1_score[cl] = metrics.f1_score(y_class_test[cl], nb_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", nb_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "l8woVPV0CUut"
      },
      "id": "l8woVPV0CUut",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, nb_f1_score, nb_scores, class_requirements)"
      ],
      "metadata": {
        "id": "lhBk8tZxDalT"
      },
      "id": "lhBk8tZxDalT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZiAylV4nNZ_I"
      },
      "id": "ZiAylV4nNZ_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic regression"
      ],
      "metadata": {
        "id": "0lF8NYv5DuHh"
      },
      "id": "0lF8NYv5DuHh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store linear regression classifiers\n",
        "lr_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a linear regression classifier for the current task\n",
        "    lr_classifier = LogisticRegression()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    lr_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    lr_classifiers[cl] = lr_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "lr_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "lr_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "lr_precision = {}\n",
        "lr_recall = {}\n",
        "lr_f1_score = {}\n",
        "lr_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    \n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    lr_pred[cl] = lr_classifiers[cl].predict(hv_test)\n",
        "    \n",
        "    #lr_pred[cl] = lr_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    lr_scores[cl] = metrics.accuracy_score(y_class_test[cl], lr_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", lr_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    lr_cms[cl] = metrics.confusion_matrix(y_class_test[cl], lr_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", lr_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    lr_precision[cl] = metrics.precision_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_recall[cl] = metrics.recall_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_f1_score[cl] = metrics.f1_score(y_class_test[cl], lr_pred[cl])\n",
        "    \n",
        "    #print(\"Precision for\", cl, \":\", lr_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", lr_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", lr_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "o1wUQqsFDqdt"
      },
      "id": "o1wUQqsFDqdt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, lr_f1_score, lr_scores, class_requirements)"
      ],
      "metadata": {
        "id": "FvwX_fIrEHBA"
      },
      "id": "FvwX_fIrEHBA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random forest"
      ],
      "metadata": {
        "id": "LTf4k64ZEKj1"
      },
      "id": "LTf4k64ZEKj1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Random Forest classifiers\n",
        "rf_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Random Forest classifier for the current task\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    rf_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    rf_classifiers[cl] = rf_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "rf_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "rf_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "rf_precision = {}\n",
        "rf_recall = {}\n",
        "rf_f1_score = {}\n",
        "rf_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    rf_pred[cl] = rf_classifiers[cl].predict(hv_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #rf_pred[cl] = rf_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    rf_scores[cl] = metrics.accuracy_score(y_class_test[cl], rf_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", rf_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    rf_cms[cl] = metrics.confusion_matrix(y_class_test[cl], rf_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", rf_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    rf_precision[cl] = metrics.precision_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_recall[cl] = metrics.recall_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_f1_score[cl] = metrics.f1_score(y_class_test[cl], rf_pred[cl])\n",
        "\n",
        "    # Print precision, recall, and F1-score for the current task\n",
        "    #print(\"Precision for\", cl, \":\", rf_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", rf_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", rf_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "59zJDgbFEJ19"
      },
      "id": "59zJDgbFEJ19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, rf_f1_score, rf_scores, class_requirements)"
      ],
      "metadata": {
        "id": "XUT1aQulGfRH"
      },
      "id": "XUT1aQulGfRH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN"
      ],
      "metadata": {
        "id": "RfyJ7Hv6Gh1G"
      },
      "id": "RfyJ7Hv6Gh1G"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store KNN classifiers\n",
        "knn_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a KNN classifier for the current task\n",
        "    knn_classifier = KNeighborsClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    knn_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    knn_classifiers[cl] = knn_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "knn_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "knn_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "knn_precision = {}\n",
        "knn_recall = {}\n",
        "knn_f1_score = {}\n",
        "knn_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    knn_pred[cl] = knn_classifiers[cl].predict(hv_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #knn_pred[cl] = knn_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    knn_scores[cl] = metrics.accuracy_score(y_class_test[cl], knn_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", knn_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    knn_cms[cl] = metrics.confusion_matrix(y_class_test[cl], knn_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", knn_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    knn_precision[cl] = metrics.precision_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_recall[cl] = metrics.recall_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_f1_score[cl] = metrics.f1_score(y_class_test[cl], knn_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "gzezthA0Gg9U"
      },
      "id": "gzezthA0Gg9U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, knn_f1_score, knn_scores, class_requirements)"
      ],
      "metadata": {
        "id": "bboRBvA2HI6F"
      },
      "id": "bboRBvA2HI6F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision tree"
      ],
      "metadata": {
        "id": "ExAv9WRrHY10"
      },
      "id": "ExAv9WRrHY10"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a dictionary to store Decision Tree classifiers\n",
        "dtc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Decision Tree classifier for the current task\n",
        "    dtc_classifier = DecisionTreeClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    dtc_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    dtc_classifiers[cl] = dtc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "dtc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "dtc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "dtc_precision = {}\n",
        "dtc_recall = {}\n",
        "dtc_f1_score = {}\n",
        "dtc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    dtc_pred[cl] = dtc_classifiers[cl].predict(hv_test)\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #dtc_pred[cl] = dtc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    dtc_scores[cl] = metrics.accuracy_score(y_class_test[cl], dtc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", dtc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    dtc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], dtc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", dtc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    dtc_precision[cl] = metrics.precision_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_recall[cl] = metrics.recall_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_f1_score[cl] = metrics.f1_score(y_class_test[cl], dtc_pred[cl])"
      ],
      "metadata": {
        "id": "oEdVOeSuHTP1"
      },
      "id": "oEdVOeSuHTP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, dtc_f1_score, dtc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "eTE5pkx2HnC1"
      },
      "id": "eTE5pkx2HnC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVC"
      ],
      "metadata": {
        "id": "ZDfP5FFRHokl"
      },
      "id": "ZDfP5FFRHokl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize a dictionary to store SVC classifiers\n",
        "svc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate an SVC classifier for the current task\n",
        "    svc_classifier = SVC()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    svc_classifier.fit(hv_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    svc_classifiers[cl] = svc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "svc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "svc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "svc_precision = {}\n",
        "svc_recall = {}\n",
        "svc_f1_score = {}\n",
        "svc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    hv_test = hv_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    svc_pred[cl] = svc_classifiers[cl].predict(hv_test)\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #svc_pred[cl] = svc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    svc_scores[cl] = metrics.accuracy_score(y_class_test[cl], svc_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", svc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    svc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], svc_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", svc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    svc_precision[cl] = metrics.precision_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_recall[cl] = metrics.recall_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_f1_score[cl] = metrics.f1_score(y_class_test[cl], svc_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "MXeyeaJ_Hnjl"
      },
      "id": "MXeyeaJ_Hnjl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, svc_f1_score, svc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "F_gyRw0_JODQ"
      },
      "id": "F_gyRw0_JODQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods comparison for TfidfVectorizer"
      ],
      "metadata": {
        "id": "j9oncSVZJTL9"
      },
      "id": "j9oncSVZJTL9"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title f1_scores\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, [np.mean(scores) for scores in f1_metrics], width=0.6, color=['red', 'green', 'blue', 'purple', 'skyblue', 'darkblue'])\n",
        "plt.title('Methods F1 Score')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ucoT9JvGJQWC"
      },
      "id": "ucoT9JvGJQWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Methods Performance Comparison for CountVectorizer\n",
        "# Create a list to store the F1-scores\n",
        "scores_ = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "scores_.append(list(nb_scores.values()))\n",
        "scores_.append(list(lr_scores.values()))\n",
        "scores_.append(list(knn_scores.values()))\n",
        "scores_.append(list(dtc_scores.values()))\n",
        "scores_.append(list(svc_scores.values()))\n",
        "scores_.append(list(rf_scores.values()))\n",
        "\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "bar_width = 0.35\n",
        "offset = np.arange(len(classifiers))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the accuracy scores\n",
        "plt.bar(offset - bar_width/2, [np.mean(scores) for scores in scores_], width=bar_width, label='Accuracy', color='blue')\n",
        "\n",
        "# Plot the F1-scores\n",
        "plt.bar(offset + bar_width/2, [np.mean(scores) for scores in f1_metrics], width=bar_width, label='F1-score', color='red')\n",
        "\n",
        "plt.title('Methods Performance Comparison')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(offset, classifiers)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KouMaO7OJZm9"
      },
      "id": "KouMaO7OJZm9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gensim TFIDF"
      ],
      "metadata": {
        "id": "FK5RIKTBLJeQ"
      },
      "id": "FK5RIKTBLJeQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###initialization"
      ],
      "metadata": {
        "id": "vB1nmOsAQEXJ"
      },
      "id": "vB1nmOsAQEXJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "metadata": {
        "id": "PjMSgA7ALPew"
      },
      "id": "PjMSgA7ALPew",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "cY5ueA-vJaFO"
      },
      "id": "cY5ueA-vJaFO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term_max_document_frequency = 2**10 #@param"
      ],
      "metadata": {
        "id": "7InLfabFMGaH"
      },
      "id": "7InLfabFMGaH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store TF-IDF vectorizers and transformed data\n",
        "genTFIDF_vectorizers = dict()\n",
        "genTFIDF_train = dict()\n",
        "genTFIDF_test = dict()\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a TfidfVectorizer for the current task\n",
        "    genTFIDF_vectorizers[cl] = TfidfVectorizer(stop_words=\"english\", max_df=term_max_document_frequency)\n",
        "\n",
        "    # Transform the training data using the TfidfVectorizer\n",
        "    genTFIDF_train[cl] = genTFIDF_vectorizers[cl].fit_transform(X_class_train[cl])\n",
        "\n",
        "    # Transform the test data using the TfidfVectorizer\n",
        "    genTFIDF_test[cl] = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n"
      ],
      "metadata": {
        "id": "fgsmOU-ELMgN"
      },
      "id": "fgsmOU-ELMgN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive bias"
      ],
      "metadata": {
        "id": "vhGHIz08MokF"
      },
      "id": "vhGHIz08MokF"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize a dictionary to store naive bias classifiers\n",
        "nb_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Bernoulli Naive Bayes classifier for the current task\n",
        "    nb_classifier = BernoulliNB() # AS MultinomialNB cannot work with negative values\n",
        "\n",
        "    # Fit the classifier to the training data for the current task\n",
        "    nb_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "\n",
        "    # Store the classifier in the dictionary\n",
        "    nb_classifiers[cl] = nb_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "nb_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "nb_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "nb_precision = {}\n",
        "nb_recall = {}\n",
        "nb_f1_score = {}\n",
        "nb_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    nb_pred[cl] = nb_classifiers[cl].predict(genTFIDF_test)\n",
        "\n",
        "    # Calculate the accuracy score for the current task\n",
        "    nb_scores[cl] = metrics.accuracy_score(y_class_test[cl], nb_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", nb_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    nb_cms[cl] = metrics.confusion_matrix(y_class_test[cl], nb_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", nb_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    nb_precision[cl] = metrics.precision_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_recall[cl] = metrics.recall_score(y_class_test[cl], nb_pred[cl])\n",
        "    nb_f1_score[cl] = metrics.f1_score(y_class_test[cl], nb_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", nb_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "b7iX0T2oJXf6"
      },
      "id": "b7iX0T2oJXf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, nb_f1_score, nb_scores, class_requirements)"
      ],
      "metadata": {
        "id": "XhcvknxNMzVP"
      },
      "id": "XhcvknxNMzVP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic regression"
      ],
      "metadata": {
        "id": "gWqrKMlDNgY1"
      },
      "id": "gWqrKMlDNgY1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store linear regression classifiers\n",
        "lr_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a linear regression classifier for the current task\n",
        "    lr_classifier = LogisticRegression()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    lr_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    lr_classifiers[cl] = lr_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "lr_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "lr_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "lr_precision = {}\n",
        "lr_recall = {}\n",
        "lr_f1_score = {}\n",
        "lr_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    \n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    lr_pred[cl] = lr_classifiers[cl].predict(genTFIDF_test)\n",
        "    \n",
        "    #lr_pred[cl] = lr_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    lr_scores[cl] = metrics.accuracy_score(y_class_test[cl], lr_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", lr_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    lr_cms[cl] = metrics.confusion_matrix(y_class_test[cl], lr_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", lr_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    lr_precision[cl] = metrics.precision_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_recall[cl] = metrics.recall_score(y_class_test[cl], lr_pred[cl])\n",
        "    lr_f1_score[cl] = metrics.f1_score(y_class_test[cl], lr_pred[cl])\n",
        "    \n",
        "    #print(\"Precision for\", cl, \":\", lr_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", lr_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", lr_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "0ch5b0ZkM2kf"
      },
      "id": "0ch5b0ZkM2kf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, lr_f1_score, lr_scores, class_requirements)"
      ],
      "metadata": {
        "id": "LVeaM466Npj4"
      },
      "id": "LVeaM466Npj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random forest"
      ],
      "metadata": {
        "id": "nO9zuBv7NtNN"
      },
      "id": "nO9zuBv7NtNN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Random Forest classifiers\n",
        "rf_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Random Forest classifier for the current task\n",
        "    rf_classifier = RandomForestClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    rf_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    rf_classifiers[cl] = rf_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "rf_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "rf_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "rf_precision = {}\n",
        "rf_recall = {}\n",
        "rf_f1_score = {}\n",
        "rf_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    rf_pred[cl] = rf_classifiers[cl].predict(genTFIDF_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #rf_pred[cl] = rf_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    rf_scores[cl] = metrics.accuracy_score(y_class_test[cl], rf_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", rf_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    rf_cms[cl] = metrics.confusion_matrix(y_class_test[cl], rf_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", rf_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    rf_precision[cl] = metrics.precision_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_recall[cl] = metrics.recall_score(y_class_test[cl], rf_pred[cl])\n",
        "    rf_f1_score[cl] = metrics.f1_score(y_class_test[cl], rf_pred[cl])\n",
        "\n",
        "    # Print precision, recall, and F1-score for the current task\n",
        "    #print(\"Precision for\", cl, \":\", rf_precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", rf_recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", rf_f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "apvdehRWNvER"
      },
      "id": "apvdehRWNvER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, rf_f1_score, rf_scores, class_requirements)"
      ],
      "metadata": {
        "id": "PmhaOtJjNsoZ"
      },
      "id": "PmhaOtJjNsoZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KNN"
      ],
      "metadata": {
        "id": "5DQuhYv3N_1Q"
      },
      "id": "5DQuhYv3N_1Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store KNN classifiers\n",
        "knn_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a KNN classifier for the current task\n",
        "    knn_classifier = KNeighborsClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    knn_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    knn_classifiers[cl] = knn_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "knn_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "knn_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "knn_precision = {}\n",
        "knn_recall = {}\n",
        "knn_f1_score = {}\n",
        "knn_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    knn_pred[cl] = knn_classifiers[cl].predict(genTFIDF_test)\n",
        "    \n",
        "    # Predict the tags for the test data of the current task\n",
        "    #knn_pred[cl] = knn_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    knn_scores[cl] = metrics.accuracy_score(y_class_test[cl], knn_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", knn_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    knn_cms[cl] = metrics.confusion_matrix(y_class_test[cl], knn_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", knn_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    knn_precision[cl] = metrics.precision_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_recall[cl] = metrics.recall_score(y_class_test[cl], knn_pred[cl])\n",
        "    knn_f1_score[cl] = metrics.f1_score(y_class_test[cl], knn_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "oKBZdaWgOA1r"
      },
      "id": "oKBZdaWgOA1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, knn_f1_score, knn_scores, class_requirements)"
      ],
      "metadata": {
        "id": "NHQ-Hs9MOBBi"
      },
      "id": "NHQ-Hs9MOBBi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision tree"
      ],
      "metadata": {
        "id": "Aao6Wd2dOaEb"
      },
      "id": "Aao6Wd2dOaEb"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize a dictionary to store Decision Tree classifiers\n",
        "dtc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate a Decision Tree classifier for the current task\n",
        "    dtc_classifier = DecisionTreeClassifier()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    dtc_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    dtc_classifiers[cl] = dtc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "dtc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "dtc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "dtc_precision = {}\n",
        "dtc_recall = {}\n",
        "dtc_f1_score = {}\n",
        "dtc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    dtc_pred[cl] = dtc_classifiers[cl].predict(genTFIDF_test)\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #dtc_pred[cl] = dtc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    dtc_scores[cl] = metrics.accuracy_score(y_class_test[cl], dtc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", dtc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    dtc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], dtc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", dtc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    dtc_precision[cl] = metrics.precision_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_recall[cl] = metrics.recall_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_f1_score[cl] = metrics.f1_score(y_class_test[cl], dtc_pred[cl])"
      ],
      "metadata": {
        "id": "5GVNhySZOc5T"
      },
      "id": "5GVNhySZOc5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, dtc_f1_score, dtc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "M2hqaf1wOjxN"
      },
      "id": "M2hqaf1wOjxN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Support vectors classification"
      ],
      "metadata": {
        "id": "UrvqEW57Opfa"
      },
      "id": "UrvqEW57Opfa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize a dictionary to store SVC classifiers\n",
        "svc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Instantiate an SVC classifier for the current task\n",
        "    svc_classifier = SVC()\n",
        "  \n",
        "    # Fit the classifier to the training data for the current task\n",
        "    svc_classifier.fit(genTFIDF_train[cl], y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    svc_classifiers[cl] = svc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "svc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "svc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "svc_precision = {}\n",
        "svc_recall = {}\n",
        "svc_f1_score = {}\n",
        "svc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Transform the test data using TfidfVectorizer\n",
        "    genTFIDF_test = genTFIDF_vectorizers[cl].transform(X_class_test[cl])\n",
        "\n",
        "    # Predict the tags for the test data of the current task\n",
        "    svc_pred[cl] = svc_classifiers[cl].predict(genTFIDF_test)\n",
        "    # Predict the tags for the test data of the current task\n",
        "    #svc_pred[cl] = svc_classifiers[cl].predict(tfidf_test[cl])\n",
        "  \n",
        "    # Calculate the accuracy score for the current task\n",
        "    svc_scores[cl] = metrics.accuracy_score(y_class_test[cl], svc_pred[cl])\n",
        "    #print(\"Accuracy score for\", cl, \":\", svc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    svc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], svc_pred[cl], labels=[1, 0])\n",
        "    #print(\"Confusion matrix for\", cl, \":\\n\", svc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    svc_precision[cl] = metrics.precision_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_recall[cl] = metrics.recall_score(y_class_test[cl], svc_pred[cl])\n",
        "    svc_f1_score[cl] = metrics.f1_score(y_class_test[cl], svc_pred[cl])\n",
        "\n",
        "    #print(\"Precision for\", cl, \":\", precision[cl])\n",
        "    #print(\"Recall for\", cl, \":\", recall[cl])\n",
        "    #print(\"F1-score for\", cl, \":\", f1_score[cl], '\\n')\n"
      ],
      "metadata": {
        "id": "qgbQ4yg5Oo8H"
      },
      "id": "qgbQ4yg5Oo8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scores_f1_accuracy(requirement_classes, svc_f1_score, svc_scores, class_requirements)"
      ],
      "metadata": {
        "id": "PmN-mifOOPxV"
      },
      "id": "PmN-mifOOPxV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods comparison for TfidfVectorizer"
      ],
      "metadata": {
        "id": "d-NzquGhPxIw"
      },
      "id": "d-NzquGhPxIw"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title f1_scores\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.bar(classifiers, [np.mean(scores) for scores in f1_metrics], width=0.6, color=['red', 'green', 'blue', 'purple', 'skyblue', 'darkblue'])\n",
        "plt.title('Methods F1 Score')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "37v7zzpKPz2L"
      },
      "id": "37v7zzpKPz2L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Methods Performance Comparison for CountVectorizer\n",
        "# Create a list to store the F1-scores\n",
        "scores_ = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "scores_.append(list(nb_scores.values()))\n",
        "scores_.append(list(lr_scores.values()))\n",
        "scores_.append(list(knn_scores.values()))\n",
        "scores_.append(list(dtc_scores.values()))\n",
        "scores_.append(list(svc_scores.values()))\n",
        "scores_.append(list(rf_scores.values()))\n",
        "\n",
        "# Create a list to store the F1-scores\n",
        "f1_metrics = []\n",
        "\n",
        "# Append the F1-scores for each classifier to the list\n",
        "f1_metrics.append(list(nb_f1_score.values()))\n",
        "f1_metrics.append(list(lr_f1_score.values()))\n",
        "f1_metrics.append(list(knn_f1_score.values()))\n",
        "f1_metrics.append(list(dtc_f1_score.values()))\n",
        "f1_metrics.append(list(svc_f1_score.values()))\n",
        "f1_metrics.append(list(rf_f1_score.values()))\n",
        "\n",
        "bar_width = 0.35\n",
        "offset = np.arange(len(classifiers))\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the accuracy scores\n",
        "plt.bar(offset - bar_width/2, [np.mean(scores) for scores in scores_], width=bar_width, label='Accuracy', color='blue')\n",
        "\n",
        "# Plot the F1-scores\n",
        "plt.bar(offset + bar_width/2, [np.mean(scores) for scores in f1_metrics], width=bar_width, label='F1-score', color='red')\n",
        "\n",
        "plt.title('Methods Performance Comparison')\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(offset, classifiers)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VIMTDs4JP0Yt"
      },
      "id": "VIMTDs4JP0Yt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT"
      ],
      "metadata": {
        "id": "_GoWtKGOQK-p"
      },
      "id": "_GoWtKGOQK-p"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urllib3==1.26.6 "
      ],
      "metadata": {
        "id": "OPPr1cubQMAJ"
      },
      "id": "OPPr1cubQMAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # Replace with the specific BERT model you want to use\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Initialize dictionaries to store BERT embeddings\n",
        "bert_embeddings_train = {}\n",
        "bert_embeddings_test = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Tokenize the training data\n",
        "    tokenized_train = [tokenizer.encode(text, add_special_tokens=True) for text in X_class_train[cl]]\n",
        "\n",
        "    # Tokenize the test data\n",
        "    tokenized_test = [tokenizer.encode(text, add_special_tokens=True) for text in X_class_test[cl]]\n",
        "\n",
        "    # Pad the tokenized sequences\n",
        "    max_length = max(max(len(seq) for seq in tokenized_train), max(len(seq) for seq in tokenized_test))\n",
        "    padded_train = [seq + [0] * (max_length - len(seq)) for seq in tokenized_train]\n",
        "    padded_test = [seq + [0] * (max_length - len(seq)) for seq in tokenized_test]\n",
        "\n",
        "    # Convert the padded sequences to tensors\n",
        "    input_ids_train = torch.tensor(padded_train)\n",
        "    input_ids_test = torch.tensor(padded_test)\n",
        "\n",
        "    # Generate BERT embeddings for the training data\n",
        "    with torch.no_grad():\n",
        "        train_outputs = model(input_ids_train)\n",
        "        bert_embeddings_train[cl] = train_outputs[0].numpy()\n",
        "\n",
        "    # Generate BERT embeddings for the test data\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(input_ids_test)\n",
        "        bert_embeddings_test[cl] = test_outputs[0].numpy()\n"
      ],
      "metadata": {
        "id": "U4aLS9EWRaGA"
      },
      "id": "U4aLS9EWRaGA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#@title Save the BERT embeddings to disk\n",
        "with open('bert_embeddings_train.pkl', 'wb') as f:\n",
        "    pickle.dump(bert_embeddings_train, f)\n",
        "\n",
        "with open('bert_embeddings_test.pkl', 'wb') as f:\n",
        "    pickle.dump(bert_embeddings_test, f)\n"
      ],
      "metadata": {
        "id": "unV_bUaaaDac"
      },
      "id": "unV_bUaaaDac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title import BERT embedings from disk\n",
        "\n",
        "# Specify the path to the saved pickle file\n",
        "train_file_path = \"bert_embeddings_train.pkl\"\n",
        "test_file_path = \"bert_embeddings_train.pkl\"\n",
        "\n",
        "# Load the pickle file\n",
        "with open(train_file_path, \"rb\") as file:\n",
        "    bert_embeddings_train = pickle.load(file)\n",
        "with open(test_file_path, \"rb\") as file:   \n",
        "    bert_embeddings_test = pickle.load(file)\n",
        "\n",
        "# Now you can use the loaded `bert_embeddings_train` in your code"
      ],
      "metadata": {
        "id": "p7k1FNynaEXH"
      },
      "id": "p7k1FNynaEXH",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store Decision Tree classifiers\n",
        "dtc_classifiers = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Flatten the BERT embeddings for the current task\n",
        "    flattened_train = bert_embeddings_train[cl].reshape(len(bert_embeddings_train[cl]), -1)\n",
        "  \n",
        "    # Instantiate a Decision Tree classifier for the current task\n",
        "    dtc_classifier = DecisionTreeClassifier()\n",
        "  \n",
        "    # Fit the classifier to the flattened training data for the current task\n",
        "    dtc_classifier.fit(flattened_train, y_class_train[cl])\n",
        "  \n",
        "    # Store the classifier in the dictionary\n",
        "    dtc_classifiers[cl] = dtc_classifier\n",
        "\n",
        "# Create a dictionary to store predicted tags for each classification task\n",
        "dtc_pred = {}\n",
        "\n",
        "# Create a dictionary to store accuracy scores for each classification task\n",
        "dtc_scores = {}\n",
        "\n",
        "# Create dictionaries to store precision, recall, and F1-score for each classification task\n",
        "dtc_precision = {}\n",
        "dtc_recall = {}\n",
        "dtc_f1_score = {}\n",
        "dtc_cms = {}\n",
        "\n",
        "# Iterate over each classification task\n",
        "for cl in requirement_classes:\n",
        "    # Flatten the BERT embeddings for the test data of the current task\n",
        "    flattened_test = bert_embeddings_test[cl].reshape(len(bert_embeddings_test[cl]), -1)\n",
        "\n",
        "    # Predict the tags for the flattened test data of the current task\n",
        "    dtc_pred[cl] = dtc_classifiers[cl].predict(flattened_test)\n",
        "\n",
        "    # Calculate the accuracy score for the current task\n",
        "    dtc_scores[cl] = metrics.accuracy_score(y_class_test[cl], dtc_pred[cl])\n",
        "    print(\"Accuracy score for\", cl, \":\", dtc_scores[cl])\n",
        "\n",
        "    # Calculate the confusion matrix for the current task\n",
        "    dtc_cms[cl] = metrics.confusion_matrix(y_class_test[cl], dtc_pred[cl], labels=[1, 0])\n",
        "    print(\"Confusion matrix for\", cl, \":\\n\", dtc_cms[cl])\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for the current task\n",
        "    dtc_precision[cl] = metrics.precision_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_recall[cl] = metrics.recall_score(y_class_test[cl], dtc_pred[cl])\n",
        "    dtc_f1_score[cl] = metrics.f1_score(y_class_test[cl], dtc_pred[cl])\n"
      ],
      "metadata": {
        "id": "m227hOR6P3cT"
      },
      "id": "m227hOR6P3cT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HE49rGlBVueH"
      },
      "id": "HE49rGlBVueH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}